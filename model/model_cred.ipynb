{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655d9aeb-6c7b-4ad0-ab69-e05ab02bc672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: c:\\Users\\tahir\\CS484\\project\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#print(\"CWD:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da72de0-ddcf-4fa8-bd1a-52eb5a657671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in c:\\users\\tahir\\anaconda3\\lib\\site-packages (1.34.0)\n",
      "Requirement already satisfied: polars-runtime-32==1.34.0 in c:\\users\\tahir\\anaconda3\\lib\\site-packages (from polars) (1.34.0)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\tahir\\anaconda3\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\tahir\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\tahir\\anaconda3\\lib\\site-packages (from lightgbm) (1.13.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\tahir\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\tahir\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tahir\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tahir\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tahir\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tahir\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tahir\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\tahir\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\tahir\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tahir\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tahir\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\tahir\\anaconda3\\lib\\site-packages (21.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install polars \n",
    "!pip install lightgbm\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b6285f5-af7c-420d-b788-018843c827fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import glob\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "TRAIN_PARQ_DIR = \"./csv_files/train/parquet/\"\n",
    "TEST_PARQ_DIR = \"./csv_files/test/parquet/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e284b44-927f-42b0-b6c3-5da8c04abd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_base: (1526659, 5)\n",
      "test_base: (10, 4)\n"
     ]
    }
   ],
   "source": [
    "#loading base tables\n",
    "train_base = pl.read_parquet(TRAIN_PARQ_DIR + \"train_base.parquet\")\n",
    "test_base  = pl.read_parquet(TEST_PARQ_DIR + \"test_base.parquet\")\n",
    "\n",
    "print(\"train_base:\", train_base.shape)\n",
    "print(\"test_base:\", test_base.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e043d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def aggregate_by_case(df: pl.DataFrame, prefix: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate a long table to one row per case_id.\n",
    "    Numeric columns -> mean, std, min, max, sum, count.\n",
    "    Uses only Polars (no pandas / pyarrow).\n",
    "    \"\"\"\n",
    "\n",
    "    # treat all integer, float, and boolean columns as numeric\n",
    "    numeric_types = (\n",
    "        pl.Int8, pl.Int16, pl.Int32, pl.Int64,\n",
    "        pl.UInt8, pl.UInt16, pl.UInt32, pl.UInt64,\n",
    "        pl.Float32, pl.Float64,\n",
    "        pl.Boolean,\n",
    "    )\n",
    "\n",
    "    numeric_cols = []\n",
    "    for c, dt in zip(df.columns, df.dtypes):\n",
    "        if c == \"case_id\":\n",
    "            continue\n",
    "        if dt in numeric_types:\n",
    "            numeric_cols.append(c)\n",
    "\n",
    "    print(f\"[{prefix}] numeric columns: {len(numeric_cols)}\")\n",
    "\n",
    "    aggs = []\n",
    "    for c in numeric_cols:\n",
    "        aggs.extend([\n",
    "            pl.col(c).mean().alias(f\"{prefix}__{c}__mean\"),\n",
    "            pl.col(c).std().alias(f\"{prefix}__{c}__std\"),\n",
    "            pl.col(c).min().alias(f\"{prefix}__{c}__min\"),\n",
    "            pl.col(c).max().alias(f\"{prefix}__{c}__max\"),\n",
    "            pl.col(c).sum().alias(f\"{prefix}__{c}__sum\"),\n",
    "        ])\n",
    "\n",
    "    # how many rows contributed for this case_id\n",
    "    aggs.append(pl.len().alias(f\"{prefix}__row_count\"))\n",
    "\n",
    "    out = df.group_by(\"case_id\").agg(aggs)\n",
    "    print(f\"[{prefix}] aggregated shape: {out.shape}\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0974446d-4b8a-4c71-85b8-91865c429454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet_group(pattern: str, prefix: str) -> pl.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Glob parquet files and concat them.\n",
    "    We only use this for applprev1 & applprev2, which are already known to work.\n",
    "    \"\"\"\n",
    "    files = glob.glob(pattern)\n",
    "    print(f\"[{prefix}] pattern: {pattern} -> {len(files)} files\")\n",
    "\n",
    "    if not files:\n",
    "        print(f\"[{prefix}] WARNING: no files found, skipping.\\n\")\n",
    "        return None\n",
    "\n",
    "    dfs = [pl.read_parquet(f) for f in files]\n",
    "    out = pl.concat(dfs, how=\"vertical_relaxed\")\n",
    "    print(f\"[{prefix}] raw shape: {out.shape}\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9075d271-878c-4c12-9992-365346da8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PARQ_DIR = \"./csv_files/train/parquet/\"\n",
    "TEST_PARQ_DIR = \"./csv_files/test/parquet/\"\n",
    "\n",
    "groups = [\n",
    "    (\"applprev1\",\n",
    "     TRAIN_PARQ_DIR + \"train_applprev_1_*.parquet\",\n",
    "     TEST_PARQ_DIR  + \"test_applprev_1_*.parquet\"),\n",
    "\n",
    "    (\"applprev2\",\n",
    "     TRAIN_PARQ_DIR + \"train_applprev_2.parquet\",\n",
    "     TEST_PARQ_DIR  + \"test_applprev_2.parquet\"),\n",
    "\n",
    "    (\"creditA\",\n",
    "     TRAIN_PARQ_DIR + \"train_credit_bureau_a_*.parquet\",\n",
    "     TEST_PARQ_DIR  + \"test_credit_bureau_a_*.parquet\"),\n",
    "\n",
    "    (\"creditB1\",\n",
    "     TRAIN_PARQ_DIR + \"train_credit_bureau_b_1.parquet\",\n",
    "     TEST_PARQ_DIR  + \"test_credit_bureau_b_1.parquet\"),\n",
    "\n",
    "    (\"creditB2\",\n",
    "     TRAIN_PARQ_DIR + \"train_credit_bureau_b_2.parquet\",\n",
    "     TEST_PARQ_DIR  + \"test_credit_bureau_b_2.parquet\"),\n",
    "\n",
    "    (\"debit1\",\n",
    "     TRAIN_PARQ_DIR + \"train_debitcard_1.parquet\",\n",
    "     TEST_PARQ_DIR  + \"test_debitcard_1.parquet\"),\n",
    "\n",
    "    (\"deposit1\",\n",
    "     TRAIN_PARQ_DIR + \"train_deposit_1.parquet\",\n",
    "     TEST_PARQ_DIR  + \"test_deposit_1.parquet\"),\n",
    "\n",
    "    (\"other1\",\n",
    "     TRAIN_PARQ_DIR + \"train_other_1.parquet\",\n",
    "     TEST_PARQ_DIR  + \"test_other_1.parquet\"),\n",
    "\n",
    "    (\"taxA1\",\n",
    "     TRAIN_PARQ_DIR + \"train_tax_registry_a_1.parquet\",\n",
    "     TEST_PARQ_DIR  + \"test_tax_registry_a_1.parquet\"),\n",
    "\n",
    "    (\"taxB1\",\n",
    "     TRAIN_PARQ_DIR + \"train_tax_registry_b_1.parquet\",\n",
    "     TEST_PARQ_DIR  + \"test_tax_registry_b_1.parquet\"),\n",
    "\n",
    "    (\"taxC1\",\n",
    "     TRAIN_PARQ_DIR + \"train_tax_registry_c_1.parquet\",\n",
    "     TEST_PARQ_DIR  + \"test_tax_registry_c_1.parquet\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0def4f93-e21d-42b1-80d0-c3f5c6207169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing group: applprev1 ===\n",
      "[applprev1_train] pattern: ./csv_files/train/parquet/train_applprev_1_*.parquet -> 2 files\n",
      "[applprev1_train] raw shape: (6525979, 41)\n",
      "[applprev1_test] pattern: ./csv_files/test/parquet/test_applprev_1_*.parquet -> 3 files\n",
      "[applprev1_test] raw shape: (30, 41)\n",
      "[applprev1] numeric columns: 21\n",
      "[applprev1] aggregated shape: (1221522, 107)\n",
      "[applprev1] numeric columns: 21\n",
      "[applprev1] aggregated shape: (4, 107)\n",
      "[applprev1] train_df shape now: (1526659, 111)\n",
      "[applprev1] test_df  shape now: (10, 110)\n",
      "\n",
      "=== Processing group: applprev2 ===\n",
      "[applprev2_train] pattern: ./csv_files/train/parquet/train_applprev_2.parquet -> 1 files\n",
      "[applprev2_train] raw shape: (14075487, 6)\n",
      "[applprev2_test] pattern: ./csv_files/test/parquet/test_applprev_2.parquet -> 1 files\n",
      "[applprev2_test] raw shape: (10, 6)\n",
      "[applprev2] numeric columns: 2\n",
      "[applprev2] aggregated shape: (1221522, 12)\n",
      "[applprev2] numeric columns: 2\n",
      "[applprev2] aggregated shape: (1, 12)\n",
      "[applprev2] train_df shape now: (1526659, 122)\n",
      "[applprev2] test_df  shape now: (10, 121)\n",
      "\n",
      "=== Processing group: creditA ===\n",
      "[creditA_train] pattern: ./csv_files/train/parquet/train_credit_bureau_a_*.parquet -> 15 files\n"
     ]
    },
    {
     "ename": "ComputeError",
     "evalue": "schema lengths differ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mComputeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prefix, train_pattern, test_pattern \u001b[38;5;129;01min\u001b[39;00m groups:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Processing group: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     train_long \u001b[38;5;241m=\u001b[39m load_parquet_group(train_pattern, prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     test_long  \u001b[38;5;241m=\u001b[39m load_parquet_group(test_pattern,  prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train_long \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m test_long \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m, in \u001b[0;36mload_parquet_group\u001b[1;34m(pattern, prefix)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     13\u001b[0m dfs \u001b[38;5;241m=\u001b[39m [pl\u001b[38;5;241m.\u001b[39mread_parquet(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files]\n\u001b[1;32m---> 14\u001b[0m out \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mconcat(dfs, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical_relaxed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] raw shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\tahir\\anaconda3\\Lib\\site-packages\\polars\\functions\\eager.py:240\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(items, how, rechunk, parallel)\u001b[0m\n\u001b[0;32m    231\u001b[0m     out \u001b[38;5;241m=\u001b[39m wrap_df(plr\u001b[38;5;241m.\u001b[39mconcat_df(elems))\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical_relaxed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    233\u001b[0m     out \u001b[38;5;241m=\u001b[39m wrap_ldf(\n\u001b[0;32m    234\u001b[0m         plr\u001b[38;5;241m.\u001b[39mconcat_lf(\n\u001b[0;32m    235\u001b[0m             [df\u001b[38;5;241m.\u001b[39mlazy() \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m elems],\n\u001b[0;32m    236\u001b[0m             rechunk\u001b[38;5;241m=\u001b[39mrechunk,\n\u001b[0;32m    237\u001b[0m             parallel\u001b[38;5;241m=\u001b[39mparallel,\n\u001b[0;32m    238\u001b[0m             to_supertypes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    239\u001b[0m         )\n\u001b[1;32m--> 240\u001b[0m     )\u001b[38;5;241m.\u001b[39mcollect(optimizations\u001b[38;5;241m=\u001b[39mQueryOptFlags\u001b[38;5;241m.\u001b[39m_eager())\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiagonal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    243\u001b[0m     out \u001b[38;5;241m=\u001b[39m wrap_df(plr\u001b[38;5;241m.\u001b[39mconcat_df_diagonal(elems))\n",
      "File \u001b[1;32mc:\\Users\\tahir\\anaconda3\\Lib\\site-packages\\polars\\_utils\\deprecation.py:97\u001b[0m, in \u001b[0;36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min-memory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstreaming\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tahir\\anaconda3\\Lib\\site-packages\\polars\\lazyframe\\opt_flags.py:328\u001b[0m, in \u001b[0;36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m         optflags \u001b[38;5;241m=\u001b[39m cb(optflags, kwargs\u001b[38;5;241m.\u001b[39mpop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[0;32m    327\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizations\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m optflags\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tahir\\anaconda3\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2415\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[1;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[0m\n\u001b[0;32m   2413\u001b[0m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[0;32m   2414\u001b[0m callback \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_opt_callback\u001b[39m\u001b[38;5;124m\"\u001b[39m, callback)\n\u001b[1;32m-> 2415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(ldf\u001b[38;5;241m.\u001b[39mcollect(engine, callback))\n",
      "\u001b[1;31mComputeError\u001b[0m: schema lengths differ"
     ]
    }
   ],
   "source": [
    "train_df = train_base\n",
    "test_df  = test_base\n",
    "\n",
    "for prefix, train_pattern, test_pattern in groups:\n",
    "    print(f\"\\n=== Processing group: {prefix} ===\")\n",
    "    \n",
    "    train_long = load_parquet_group(train_pattern, prefix + \"_train\")\n",
    "    test_long  = load_parquet_group(test_pattern,  prefix + \"_test\")\n",
    "\n",
    "    if train_long is None or test_long is None:\n",
    "        print(f\"[{prefix}] skipped (files missing)\")\n",
    "        continue\n",
    "\n",
    "    train_agg = aggregate_by_case(train_long, prefix)\n",
    "    test_agg  = aggregate_by_case(test_long,  prefix)\n",
    "\n",
    "    # left join on case_id\n",
    "    train_df = train_df.join(train_agg, on=\"case_id\", how=\"left\")\n",
    "    test_df  = test_df.join(test_agg,  on=\"case_id\", how=\"left\")\n",
    "\n",
    "    print(f\"[{prefix}] train_df shape now: {train_df.shape}\")\n",
    "    print(f\"[{prefix}] test_df  shape now: {test_df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1801147b-d25e-4dd0-9cf7-d1244d288fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polars shapes: (1526659, 122) (10, 121)\n",
      "Pandas shapes: (1526659, 122) (10, 121)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>date_decision</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>WEEK_NUM</th>\n",
       "      <th>target</th>\n",
       "      <th>applprev1__actualdpd_943P__mean</th>\n",
       "      <th>applprev1__actualdpd_943P__std</th>\n",
       "      <th>applprev1__actualdpd_943P__min</th>\n",
       "      <th>applprev1__actualdpd_943P__max</th>\n",
       "      <th>applprev1__actualdpd_943P__sum</th>\n",
       "      <th>...</th>\n",
       "      <th>applprev2__num_group1__std</th>\n",
       "      <th>applprev2__num_group1__min</th>\n",
       "      <th>applprev2__num_group1__max</th>\n",
       "      <th>applprev2__num_group1__sum</th>\n",
       "      <th>applprev2__num_group2__mean</th>\n",
       "      <th>applprev2__num_group2__std</th>\n",
       "      <th>applprev2__num_group2__min</th>\n",
       "      <th>applprev2__num_group2__max</th>\n",
       "      <th>applprev2__num_group2__sum</th>\n",
       "      <th>applprev2__row_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>201901</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id date_decision   MONTH  WEEK_NUM  target  \\\n",
       "0        0    2019-01-03  201901         0       0   \n",
       "1        1    2019-01-03  201901         0       0   \n",
       "2        2    2019-01-04  201901         0       0   \n",
       "3        3    2019-01-03  201901         0       0   \n",
       "4        4    2019-01-04  201901         0       1   \n",
       "\n",
       "   applprev1__actualdpd_943P__mean  applprev1__actualdpd_943P__std  \\\n",
       "0                              NaN                             NaN   \n",
       "1                              NaN                             NaN   \n",
       "2                              0.0                             0.0   \n",
       "3                              0.0                             NaN   \n",
       "4                              0.0                             NaN   \n",
       "\n",
       "   applprev1__actualdpd_943P__min  applprev1__actualdpd_943P__max  \\\n",
       "0                             NaN                             NaN   \n",
       "1                             NaN                             NaN   \n",
       "2                             0.0                             0.0   \n",
       "3                             0.0                             0.0   \n",
       "4                             0.0                             0.0   \n",
       "\n",
       "   applprev1__actualdpd_943P__sum  ...  applprev2__num_group1__std  \\\n",
       "0                             NaN  ...                         NaN   \n",
       "1                             NaN  ...                         NaN   \n",
       "2                             0.0  ...                     0.57735   \n",
       "3                             0.0  ...                     0.00000   \n",
       "4                             0.0  ...                     0.00000   \n",
       "\n",
       "   applprev2__num_group1__min  applprev2__num_group1__max  \\\n",
       "0                         NaN                         NaN   \n",
       "1                         NaN                         NaN   \n",
       "2                         0.0                         1.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   applprev2__num_group1__sum  applprev2__num_group2__mean  \\\n",
       "0                         NaN                          NaN   \n",
       "1                         NaN                          NaN   \n",
       "2                         2.0                          0.5   \n",
       "3                         0.0                          1.0   \n",
       "4                         0.0                          0.5   \n",
       "\n",
       "   applprev2__num_group2__std  applprev2__num_group2__min  \\\n",
       "0                         NaN                         NaN   \n",
       "1                         NaN                         NaN   \n",
       "2                    0.577350                         0.0   \n",
       "3                    1.000000                         0.0   \n",
       "4                    0.707107                         0.0   \n",
       "\n",
       "   applprev2__num_group2__max  applprev2__num_group2__sum  \\\n",
       "0                         NaN                         NaN   \n",
       "1                         NaN                         NaN   \n",
       "2                         1.0                         2.0   \n",
       "3                         2.0                         3.0   \n",
       "4                         1.0                         1.0   \n",
       "\n",
       "   applprev2__row_count  \n",
       "0                   NaN  \n",
       "1                   NaN  \n",
       "2                   4.0  \n",
       "3                   3.0  \n",
       "4                   2.0  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Polars shapes:\", train_df.shape, test_df.shape)\n",
    "\n",
    "train_pd = pd.DataFrame(train_df.to_dicts())\n",
    "test_pd  = pd.DataFrame(test_df.to_dicts())\n",
    "\n",
    "print(\"Pandas shapes:\", train_pd.shape, test_pd.shape)\n",
    "train_pd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f25df0d-b50d-42f8-9d5d-5ea919aebf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   case_id  score\n",
      "0    57543    0.5\n",
      "1    57549    0.5\n",
      "2    57551    0.5\n",
      "3    57552    0.5\n",
      "4    57569    0.5\n",
      "ID_COL: case_id TARGET_COL: target\n"
     ]
    }
   ],
   "source": [
    "sample = pd.read_csv(\"./csv_files/sample_submission.csv\")\n",
    "print(sample.head())\n",
    "\n",
    "ID_COL = sample.columns[0]    # e.g., 'case_id'\n",
    "TARGET_COL = \"target\"         # per competition spec\n",
    "\n",
    "print(\"ID_COL:\", ID_COL, \"TARGET_COL:\", TARGET_COL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4005279-6812-41f1-ae1a-1ca94c624918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num features: 114\n",
      "X: (1526659, 114) X_test: (10, 114)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "drop_cols = [ID_COL, TARGET_COL]\n",
    "\n",
    "# optional: drop date-like columns if they exist\n",
    "for col in [\"date_decision\", \"WEEK_NUM\"]:\n",
    "    if col in train_pd.columns:\n",
    "        drop_cols.append(col)\n",
    "\n",
    "numeric_cols = train_pd.select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols = [c for c in numeric_cols if c not in drop_cols]\n",
    "\n",
    "print(\"Num features:\", len(feature_cols))\n",
    "\n",
    "X = train_pd[feature_cols]\n",
    "y = train_pd[TARGET_COL]\n",
    "X_test = test_pd[feature_cols]\n",
    "\n",
    "print(\"X:\", X.shape, \"X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05976552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8bb3f970-6950-49fc-b73a-5294ddd040df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1221327, 114) Val: (305332, 114)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62ade0a4-7215-4a0c-9299-1e27a3ce089e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[50]\ttrain's auc: 0.743465\tval's auc: 0.722636\n",
      "[100]\ttrain's auc: 0.760512\tval's auc: 0.727661\n",
      "[150]\ttrain's auc: 0.774097\tval's auc: 0.729171\n",
      "[200]\ttrain's auc: 0.784999\tval's auc: 0.729505\n",
      "[250]\ttrain's auc: 0.794169\tval's auc: 0.729858\n",
      "[300]\ttrain's auc: 0.803342\tval's auc: 0.729933\n",
      "[350]\ttrain's auc: 0.810145\tval's auc: 0.729914\n",
      "Early stopping, best iteration is:\n",
      "[275]\ttrain's auc: 0.798736\tval's auc: 0.730099\n",
      "Validation AUC: 0.7300994589091913\n"
     ]
    }
   ],
   "source": [
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data   = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 63,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, val_data],\n",
    "    valid_names=[\"train\", \"val\"],\n",
    "    num_boost_round=2000,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100),\n",
    "        lgb.log_evaluation(period=50),\n",
    "    ],\n",
    ")\n",
    "\n",
    "val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "val_auc  = roc_auc_score(y_val, val_pred)\n",
    "print(\"Validation AUC:\", val_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5023ca24-95a4-4ac1-9757-31ff0e5bc4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.08348859070624812\n",
      "\n",
      "=== Metrics at threshold 0.5 ===\n",
      "Accuracy: 0.9685653649142573\n",
      "Recall: 0.0007292426294405667\n",
      "Precision: 0.5384615384615384\n",
      "F1: 0.0014565126924677486\n",
      "\n",
      "=== Metrics at best threshold ===\n",
      "Accuracy: 0.9212365556181468\n",
      "Recall: 0.25523492030419836\n",
      "Precision: 0.12661498708010335\n",
      "F1: 0.16926318698400636\n",
      "\n",
      "PR-AUC: 0.09903058380934145\n"
     ]
    }
   ],
   "source": [
    "# Precision-recall curve to choose threshold\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, val_pred)\n",
    "\n",
    "# Avoid division by zero\n",
    "f1s = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "best_idx = np.argmax(f1s)\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(\"Best threshold:\", best_threshold)\n",
    "\n",
    "# Metrics at default 0.5\n",
    "val_bin_05 = (val_pred > 0.5).astype(int)\n",
    "acc_05 = accuracy_score(y_val, val_bin_05)\n",
    "rec_05 = recall_score(y_val, val_bin_05)\n",
    "prec_05 = precision_score(y_val, val_bin_05)\n",
    "f1_05 = f1_score(y_val, val_bin_05)\n",
    "\n",
    "print(\"\\n=== Metrics at threshold 0.5 ===\")\n",
    "print(\"Accuracy:\", acc_05)\n",
    "print(\"Recall:\",   rec_05)\n",
    "print(\"Precision:\",prec_05)\n",
    "print(\"F1:\",       f1_05)\n",
    "\n",
    "# Metrics at best_threshold\n",
    "val_bin_best = (val_pred > best_threshold).astype(int)\n",
    "acc_best = accuracy_score(y_val, val_bin_best)\n",
    "rec_best = recall_score(y_val, val_bin_best)\n",
    "prec_best = precision_score(y_val, val_bin_best)\n",
    "f1_best = f1_score(y_val, val_bin_best)\n",
    "\n",
    "print(\"\\n=== Metrics at best threshold ===\")\n",
    "print(\"Accuracy:\", acc_best)\n",
    "print(\"Recall:\",   rec_best)\n",
    "print(\"Precision:\",prec_best)\n",
    "print(\"F1:\",       f1_best)\n",
    "\n",
    "# PR-AUC\n",
    "pr_auc = average_precision_score(y_val, val_pred)\n",
    "print(\"\\nPR-AUC:\", pr_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7902ece3-d30d-4f0e-92a3-f553c238b080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57543</td>\n",
       "      <td>0.036672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57549</td>\n",
       "      <td>0.027360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57551</td>\n",
       "      <td>0.014639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57552</td>\n",
       "      <td>0.014639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57569</td>\n",
       "      <td>0.014639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id     score\n",
       "0    57543  0.036672\n",
       "1    57549  0.027360\n",
       "2    57551  0.014639\n",
       "3    57552  0.014639\n",
       "4    57569  0.014639"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    ID_COL: test_pd[ID_COL],\n",
    "    \"score\": test_pred,\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_full_parquet_model.csv\", index=False)\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc0584ba-2556-40a0-8055-a8cebe50ceb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.08348859070624812\n",
      "Accuracy: 0.9212365556181468\n",
      "Recall: 0.25523492030419836\n",
      "Precision: 0.12661498708010335\n",
      "F1: 0.16926318698400636\n",
      "PR-AUC: 0.09903058380934145\n"
     ]
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_val, val_pred)\n",
    "f1s = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "best_idx = np.argmax(f1s)\n",
    "best_threshold = thresholds[best_idx]\n",
    "print(\"Best threshold:\", best_threshold)\n",
    "\n",
    "val_bin_best = (val_pred > best_threshold).astype(int)\n",
    "print(\"Accuracy:\",      accuracy_score(y_val, val_bin_best))\n",
    "print(\"Recall:\",        recall_score(y_val, val_bin_best))\n",
    "print(\"Precision:\",     precision_score(y_val, val_bin_best))\n",
    "print(\"F1:\",            f1_score(y_val, val_bin_best))\n",
    "print(\"PR-AUC:\",        average_precision_score(y_val, val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bdd8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
